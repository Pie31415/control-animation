{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with Text-To-Video Zero Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import imageio\n",
    "from diffusers import TextToVideoZeroPipeline, ControlNetModel, StableDiffusionControlNetPipeline, TextToVideoZeroPipeline\n",
    "from diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor\n",
    "from huggingface_hub import hf_hub_download\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "jax.local_devices()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text-To-Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"runwayml/stable-diffusion-v1-5\" # TODO: experiment with pretrained custom models on hugging face\n",
    "pipe = TextToVideoZeroPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "\n",
    "prompt = \"A panda is playing guitar on times square\"\n",
    "result = pipe(prompt=prompt).images\n",
    "result = [(r * 255).astype(\"uint8\") for r in result]\n",
    "imageio.mimsave(\"video.mp4\", result, fps=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text-To-Video with Pose Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"runwayml/stable-diffusion-v1-5\" # base model\n",
    "repo_id = \"PAIR/Text2Video-Zero\"\n",
    "video_path = \"__assets__/poses_skeleton_gifs/dance1_corr.mp4\" # pose video\n",
    "\n",
    "reader = imageio.get_reader(video_path, \"ffmpeg\")\n",
    "frame_count = 8\n",
    "pose_images = [Image.fromarray(reader.get_data(i)) for i in range(frame_count)]\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-openpose\")\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(model_id, controlnet=controlnet)\n",
    "\n",
    "# Set the attention processor\n",
    "pipe.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))\n",
    "pipe.controlnet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))\n",
    "\n",
    "# fix latents for all frames\n",
    "latents = torch.randn((1, 4, 64, 64)).repeat(len(pose_images), 1, 1, 1)\n",
    "\n",
    "prompt = \"Darth Vader dancing in a desert\"\n",
    "result = pipe(prompt=[prompt] * len(pose_images), image=pose_images, latents=latents).images\n",
    "imageio.mimsave(\"video.mp4\", result, fps=4)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text-To-Video with Safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
